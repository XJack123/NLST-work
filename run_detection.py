#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
NLSTè‚ºç»“èŠ‚æ£€æµ‹ç‹¬ç«‹è¿è¡Œè„šæœ¬
åªéœ€è¿è¡Œè¿™ä¸ªæ–‡ä»¶å³å¯å®Œæˆæ£€æµ‹
"""

import json
import logging
import sys
import time
import os
import numpy as np
import torch

import monai
from monai.apps.detection.networks.retinanet_detector import RetinaNetDetector
from monai.apps.detection.utils.anchor_utils import AnchorGeneratorWithAnchorShape
from monai.data import DataLoader, Dataset
from monai.data.utils import no_collation
from monai.transforms import (
    Compose,
    EnsureChannelFirstd,
    LoadImaged,
    Orientationd,
    ScaleIntensityRanged,
    EnsureTyped,
    ToTensord,
    Invertd,
    DeleteItemsd,
    SelectItemsd,
    Lambda,
    Lambdad,
)

print("=" * 80)
print("NLSTè‚ºç»“èŠ‚æ£€æµ‹ç³»ç»Ÿ")
print("=" * 80)

# ==================== é…ç½®å‚æ•° ====================
CONFIG = {
    "environment": {
        "data_base_dir": "./NLSTNIfTIdata/",
        "data_list_file_path": "./nlst_detection_datalist.json",
        "model_path": "./models/DLCSD-mD.pt",
        "result_list_file_path": "./nlst_detection_results.json",
    },
    "model": {
        "gt_box_mode": "cccwhd",
        "val_patch_size": [512, 512, 208],
        "score_thresh": 0.02,
        "nms_thresh": 0.22,
        "returned_layers": [1, 2],
        "base_anchor_shapes": [[6, 8, 4], [8, 6, 5], [10, 10, 6]],
    }
}

print("\nğŸ“‹ é…ç½®ä¿¡æ¯:")
print(f"  æ•°æ®ç›®å½•: {CONFIG['environment']['data_base_dir']}")
print(f"  æ¨¡å‹æ–‡ä»¶: {CONFIG['environment']['model_path']}")
print(f"  ç»“æœè¾“å‡º: {CONFIG['environment']['result_list_file_path']}")
print(f"  Patchå¤§å°: {CONFIG['model']['val_patch_size']}")

# ==================== æ£€æŸ¥æ–‡ä»¶ ====================
def check_files():
    """æ£€æŸ¥å¿…éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("\nğŸ” æ£€æŸ¥å¿…éœ€æ–‡ä»¶...")
    
    required_files = [
        CONFIG['environment']['data_list_file_path'],
        CONFIG['environment']['model_path'],
    ]
    
    missing_files = []
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"  âœ… {file_path}")
        else:
            print(f"  âŒ {file_path} (ç¼ºå¤±)")
            missing_files.append(file_path)
    
    if missing_files:
        print(f"\nâŒ é”™è¯¯: ä»¥ä¸‹æ–‡ä»¶ç¼ºå¤±:")
        for f in missing_files:
            print(f"  - {f}")
        sys.exit(1)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if os.path.exists(CONFIG['environment']['data_base_dir']):
        print(f"  âœ… {CONFIG['environment']['data_base_dir']}")
    else:
        print(f"  âŒ {CONFIG['environment']['data_base_dir']} (ç¼ºå¤±)")
        sys.exit(1)
    
    print("âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ£€æŸ¥é€šè¿‡!")

# ==================== å®šä¹‰Transforms ====================
def generate_detection_inference_transform(
    image_key,
    pred_box_key,
    pred_label_key,
    pred_score_key,
    gt_box_mode,
    intensity_transform,
    affine_lps_to_ras=False,
    amp=True,
):
    """ç”Ÿæˆæ£€æµ‹æ¨ç†çš„transform"""
    
    # æ¨ç†transforms
    inference_transforms = Compose([
        LoadImaged(keys=[image_key], image_only=False),  # ä¿ç•™å…ƒæ•°æ®
        EnsureChannelFirstd(keys=[image_key]),
        # å¼ºåˆ¶é€‰æ‹©ç¬¬ä¸€ä¸ªé€šé“ï¼ˆå¤„ç†å¤šé€šé“å›¾åƒï¼‰
        Lambdad(keys=[image_key], func=lambda x: x[:1] if x.shape[0] > 1 else x),
        # Orientationd åœ¨æŸäº›æŸåçš„NIfTIæ–‡ä»¶ä¸Šä¼šå¤±è´¥ï¼Œå·²ç§»é™¤
        # Orientationd(keys=[image_key], axcodes="RAS" if affine_lps_to_ras else "LPS"),
        intensity_transform,
        EnsureTyped(keys=[image_key], dtype=torch.float16 if amp else torch.float32),
    ])
    
    # åå¤„ç†transforms
    post_transforms = Compose([
        Invertd(
            keys=[pred_box_key],
            transform=inference_transforms,
            orig_keys=[image_key],
            nearest_interp=False,
            to_tensor=True,
        ),
        DeleteItemsd(keys=["image", "image_transforms"]),
    ])
    
    return inference_transforms, post_transforms

# ==================== ä¸»ç¨‹åº ====================
def main():
    """ä¸»ç¨‹åº"""
    
    # æ£€æŸ¥æ–‡ä»¶
    check_files()
    
    # è®¾ç½®
    amp = True
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type == "cpu":
        print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè¿è¡Œï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
    
    monai.config.print_config()
    
    # åŠ è½½é…ç½®
    env_dict = CONFIG['environment']
    config_dict = CONFIG['model']
    
    patch_size = config_dict['val_patch_size']
    
    # 1. å®šä¹‰transform
    print("\nğŸ”§ å®šä¹‰æ•°æ®è½¬æ¢...")
    intensity_transform = ScaleIntensityRanged(
        keys=["image"],
        a_min=-1024,
        a_max=300.0,
        b_min=0.0,
        b_max=1.0,
        clip=True,
    )
    
    inference_transforms, post_transforms = generate_detection_inference_transform(
        "image",
        "pred_box",
        "pred_label",
        "pred_score",
        config_dict['gt_box_mode'],
        intensity_transform,
        affine_lps_to_ras=False,
        amp=amp,
    )
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“‚ åŠ è½½æ•°æ®åˆ—è¡¨...")
    
    # æ‰‹åŠ¨åŠ è½½JSONæ–‡ä»¶ä»¥æ­£ç¡®å¤„ç†UTF-8ç¼–ç ï¼ˆåŒ…æ‹¬BOMï¼‰
    with open(env_dict['data_list_file_path'], 'r', encoding='utf-8-sig') as f:
        datalist_json = json.load(f)
    
    # å¤„ç†æ•°æ®åˆ—è¡¨
    base_dir = env_dict['data_base_dir']
    inference_data = []
    for item in datalist_json.get('validation', []):
        if isinstance(item, dict):
            # æ·»åŠ å®Œæ•´è·¯å¾„ï¼Œå¹¶æ¸…ç†å¯èƒ½çš„BOMå­—ç¬¦
            item_copy = item.copy()
            if 'image' in item_copy:
                # æ¸…ç†BOMå’Œå…¶ä»–ä¸å¯è§å­—ç¬¦
                image_path = item_copy['image'].lstrip('\ufeff').strip()
                item_copy['image'] = os.path.join(base_dir, image_path)
            inference_data.append(item_copy)
    
    print(f"  æ‰¾åˆ° {len(inference_data)} ä¸ªCTæ‰«ææ–‡ä»¶")
    
    inference_ds = Dataset(
        data=inference_data,
        transform=inference_transforms,
    )
    
    inference_loader = DataLoader(
        inference_ds,
        batch_size=1,
        num_workers=0,  # Windowsä¸Šé¿å…pickleé—®é¢˜
        pin_memory=torch.cuda.is_available(),
        collate_fn=no_collation,
    )
    
    # 3. æ„å»ºæ¨¡å‹
    print("\nğŸ—ï¸  æ„å»ºæ£€æµ‹æ¨¡å‹...")
    
    # æ„å»ºanchorç”Ÿæˆå™¨
    anchor_generator = AnchorGeneratorWithAnchorShape(
        feature_map_scales=[2**l for l in range(len(config_dict['returned_layers']) + 1)],
        base_anchor_shapes=config_dict['base_anchor_shapes'],
    )
    
    # åŠ è½½ç½‘ç»œ
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {env_dict['model_path']}")
    try:
        net = torch.jit.load(env_dict['model_path']).to(device)
        print("  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # æ„å»ºæ£€æµ‹å™¨
    detector = RetinaNetDetector(
        network=net, 
        anchor_generator=anchor_generator, 
        debug=False
    )
    
    # è®¾ç½®æ¨ç†å‚æ•°
    detector.set_box_selector_parameters(
        score_thresh=config_dict['score_thresh'],
        topk_candidates_per_level=1000,
        nms_thresh=config_dict['nms_thresh'],
        detections_per_img=100,
    )
    
    detector.set_sliding_window_inferer(
        roi_size=patch_size,
        overlap=0.25,
        sw_batch_size=4,  # å¢åŠ batch sizeä»¥å……åˆ†åˆ©ç”¨GPU
        mode="constant",
        device="cuda" if torch.cuda.is_available() else "cpu",  # ä½¿ç”¨GPUï¼
    )
    
    # 4. è¿è¡Œæ¨ç†
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹æ£€æµ‹...")
    print("=" * 80)
    
    results_dict = {"validation": []}
    detector.eval()
    
    with torch.no_grad():
        start_time = time.time()
        
        for idx, inference_data in enumerate(inference_loader, 1):
            inference_img_filenames = [
                inference_data_i["image_meta_dict"]["filename_or_obj"]
                for inference_data_i in inference_data
            ]
            
            print(f"\n[{idx}/{len(inference_loader)}] å¤„ç†: {os.path.basename(inference_img_filenames[0])}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨inferer
            use_inferer = not all([
                inference_data_i["image"][0, ...].numel() < np.prod(patch_size)
                for inference_data_i in inference_data
            ])
            
            if use_inferer:
                print("  ä½¿ç”¨sliding windowæ¨ç†ï¼ˆå›¾åƒè¾ƒå¤§ï¼‰")
            
            inference_inputs = [
                inference_data_i["image"].to(device)
                for inference_data_i in inference_data
            ]
            
            # è¿è¡Œæ£€æµ‹
            inference_start = time.time()
            if amp:
                with torch.cuda.amp.autocast():
                    inference_outputs = detector(
                        inference_inputs, use_inferer=use_inferer
                    )
            else:
                inference_outputs = detector(inference_inputs, use_inferer=use_inferer)
            
            inference_time = time.time() - inference_start
            print(f"  æ¨ç†è€—æ—¶: {inference_time:.2f}ç§’")
            
            del inference_inputs
            
            # æ›´æ–°æ¨ç†æ•°æ®è¿›è¡Œåå¤„ç†
            for i in range(len(inference_outputs)):
                inference_data_i, inference_pred_i = (
                    inference_data[i],
                    inference_outputs[i],
                )
                inference_data_i["pred_box"] = inference_pred_i[
                    detector.target_box_key
                ].to(torch.float32)
                inference_data_i["pred_label"] = inference_pred_i[
                    detector.target_label_key
                ]
                inference_data_i["pred_score"] = inference_pred_i[
                    detector.pred_score_key
                ].to(torch.float32)
                inference_data[i] = post_transforms(inference_data_i)
            
            # ä¿å­˜ç»“æœ
            for inference_img_filename, inference_pred_i in zip(
                inference_img_filenames, inference_data
            ):
                num_detections = len(inference_pred_i["pred_label"])
                print(f"  æ£€æµ‹åˆ° {num_detections} ä¸ªå€™é€‰ç»“èŠ‚")
                
                if num_detections > 0:
                    scores = inference_pred_i["pred_score"].cpu().detach().numpy()
                    high_conf = sum(scores > 0.5)
                    med_conf = sum((scores > 0.2) & (scores <= 0.5))
                    print(f"    - é«˜ç½®ä¿¡åº¦ (>0.5): {high_conf} ä¸ª")
                    print(f"    - ä¸­ç­‰ç½®ä¿¡åº¦ (0.2-0.5): {med_conf} ä¸ª")
                
                result = {
                    "label": inference_pred_i["pred_label"]
                    .cpu()
                    .detach()
                    .numpy()
                    .tolist(),
                    "box": inference_pred_i["pred_box"].cpu().detach().numpy().tolist(),
                    "score": inference_pred_i["pred_score"]
                    .cpu()
                    .detach()
                    .numpy()
                    .tolist(),
                }
                result.update({"image": inference_img_filename})
                results_dict["validation"].append(result)
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print("\n" + "=" * 80)
    print("âœ… æ£€æµ‹å®Œæˆ!")
    print("=" * 80)
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    print(f"å¹³å‡æ¯ä¸ªæ‰«æ: {total_time/len(inference_loader):.2f}ç§’")
    
    # 5. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {env_dict['result_list_file_path']}")
    with open(env_dict['result_list_file_path'], "w", encoding='utf-8') as outfile:
        json.dump(results_dict, outfile, indent=2, ensure_ascii=False)
    
    print("âœ… ç»“æœå·²ä¿å­˜!")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ£€æµ‹ç»Ÿè®¡:")
    total_detections = sum([len(r['label']) for r in results_dict['validation']])
    files_with_detections = sum([1 for r in results_dict['validation'] if len(r['label']) > 0])
    print(f"  æ€»æ£€æµ‹æ•°: {total_detections}")
    print(f"  æœ‰æ£€æµ‹ç»“æœçš„æ–‡ä»¶: {files_with_detections}/{len(results_dict['validation'])}")
    
    if total_detections > 0:
        all_scores = []
        for r in results_dict['validation']:
            all_scores.extend(r['score'])
        all_scores = np.array(all_scores)
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(all_scores):.3f}")
        print(f"  æœ€é«˜ç½®ä¿¡åº¦: {np.max(all_scores):.3f}")
        print(f"  æœ€ä½ç½®ä¿¡åº¦: {np.min(all_scores):.3f}")
    
    print("\nğŸ‰ å®Œæˆ! è¯·æŸ¥çœ‹ç»“æœæ–‡ä»¶è·å–è¯¦ç»†æ£€æµ‹ä¿¡æ¯ã€‚")
    print(f"   ç»“æœæ–‡ä»¶: {env_dict['result_list_file_path']}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

