#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è‚ºç»“èŠ‚ç‰¹å¾æå–ä¸å¯è§£é‡Šæ€§åˆ†æç³»ç»Ÿ
ä»detectionç»“æœä¸­æå–ç»“èŠ‚ï¼Œä½¿ç”¨classificationæ¨¡å‹åˆ†æç‰¹å¾
"""

import json
import os
import numpy as np
import torch
import nibabel as nib
from scipy import ndimage
import pandas as pd

import monai
from monai.networks.nets import resnet50

print("=" * 80)
print("è‚ºç»“èŠ‚ç‰¹å¾æå–ä¸å¯è§£é‡Šæ€§åˆ†æç³»ç»Ÿ")
print("=" * 80)

# ==================== é…ç½® ====================
CONFIG = {
    "detection_results": "./nlst_detection_results.json",
    "classification_model": "./models/DukeLungRADS_CancerCL_Fold1_resnet50WSPP.pt",
    "output_features": "./nodule_features.csv",
    "output_predictions": "./nodule_predictions.csv",
    "output_explanations": "./nodule_explanations/",
    
    # ROIæå–å‚æ•°
    "roi_size": [64, 64, 64],  # ç»“èŠ‚ROIå¤§å°
    "margin_factor": 2.0,  # è¾¹ç•Œæ¡†æ‰©å±•å€æ•°
    "min_confidence": 0.2,  # æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆåªåˆ†æç½®ä¿¡åº¦>0.2çš„ç»“èŠ‚ï¼‰
    
    # æ¨¡å‹å‚æ•°
    "num_classes": 2,  # è‰¯æ€§(0) vs æ¶æ€§(1)
    "spatial_dims": 3,
    "n_input_channels": 1,
}

print("\nğŸ“‹ é…ç½®ä¿¡æ¯:")
print(f"  Detectionç»“æœ: {CONFIG['detection_results']}")
print(f"  Classificationæ¨¡å‹: {CONFIG['classification_model']}")
print(f"  ROIå¤§å°: {CONFIG['roi_size']}")
print(f"  æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼: {CONFIG['min_confidence']}")

# ==================== GradCAMå®ç°ï¼ˆå¯è§£é‡Šæ€§ï¼‰ ====================
class GradCAM:
    """æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ - ç”¨äºå¯è§†åŒ–æ¨¡å‹å…³æ³¨çš„åŒºåŸŸ"""
    
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        # æ³¨å†Œhooks
        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_full_backward_hook(self.save_gradient)
    
    def save_activation(self, module, input, output):
        self.activations = output.detach()
    
    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()
    
    def generate(self, input_tensor, target_class=None):
        """ç”ŸæˆGradCAMçƒ­å›¾"""
        self.model.eval()
        
        # Forward pass
        output = self.model(input_tensor)
        
        if target_class is None:
            target_class = output.argmax(dim=1).item()
        
        # Backward pass
        self.model.zero_grad()
        class_score = output[0, target_class]
        class_score.backward()
        
        # è®¡ç®—æƒé‡
        weights = self.gradients.mean(dim=(2, 3, 4), keepdim=True)
        
        # åŠ æƒæ±‚å’Œ
        cam = (weights * self.activations).sum(dim=1, keepdim=True)
        cam = torch.relu(cam)
        
        # å½’ä¸€åŒ–åˆ°0-1
        cam = cam - cam.min()
        if cam.max() > 0:
            cam = cam / cam.max()
        
        return cam.squeeze().cpu().numpy(), target_class

# ==================== ç‰¹å¾æå–å™¨ ====================
class NoduleFeatureExtractor:
    """è‚ºç»“èŠ‚ç‰¹å¾æå–å™¨"""
    
    def __init__(self, model_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"\nğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ“¥ åŠ è½½classificationæ¨¡å‹...")
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # åˆ›å»ºç‰¹å¾æå–hook
        self.features = {}
        self._register_hooks()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    def _load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        # åˆ›å»ºResNet50æ¨¡å‹
        model = resnet50(
            pretrained=False,
            spatial_dims=CONFIG['spatial_dims'],
            n_input_channels=CONFIG['n_input_channels'],
            num_classes=CONFIG['num_classes']
        )
        
        # åŠ è½½æƒé‡
        state_dict = torch.load(model_path, map_location=self.device)
        model.load_state_dict(state_dict)
        model = model.to(self.device)
        
        return model
    
    def _register_hooks(self):
        """æ³¨å†Œç‰¹å¾æå–hooks - æå–å¤šå±‚ç‰¹å¾ï¼ˆæµ…å±‚åˆ°æ·±å±‚ï¼‰"""
        def get_features(name):
            def hook(model, input, output):
                self.features[name] = output.detach()
            return hook
        
        # ResNet50çš„å¤šå±‚ç‰¹å¾æå–
        # layer1: æµ…å±‚ç‰¹å¾ (256ç»´ç‰¹å¾å›¾ï¼Œåˆ†è¾¨ç‡è¾ƒé«˜ï¼Œæ•è·è¾¹ç¼˜ã€çº¹ç†ç­‰ä½çº§ç‰¹å¾)
        if hasattr(self.model, 'layer1'):
            self.model.layer1.register_forward_hook(get_features('layer1'))
        
        # layer2: ä¸­æµ…å±‚ç‰¹å¾ (512ç»´ç‰¹å¾å›¾ï¼Œæ•è·ç®€å•å½¢çŠ¶å’Œæ¨¡å¼)
        if hasattr(self.model, 'layer2'):
            self.model.layer2.register_forward_hook(get_features('layer2'))
        
        # layer3: ä¸­æ·±å±‚ç‰¹å¾ (1024ç»´ç‰¹å¾å›¾ï¼Œæ•è·å¤æ‚å½¢çŠ¶å’Œéƒ¨ä»¶)
        if hasattr(self.model, 'layer3'):
            self.model.layer3.register_forward_hook(get_features('layer3'))
        
        # layer4: æ·±å±‚ç‰¹å¾ (2048ç»´ç‰¹å¾å›¾ï¼ŒåŒ…å«é«˜çº§è¯­ä¹‰ç‰¹å¾)
        if hasattr(self.model, 'layer4'):
            self.model.layer4.register_forward_hook(get_features('layer4'))
        
        # avgpool: å…¨å±€å¹³å‡æ± åŒ–åçš„æœ€ç»ˆç‰¹å¾å‘é‡ (2048ç»´)
        if hasattr(self.model, 'avgpool'):
            self.model.avgpool.register_forward_hook(get_features('avgpool'))
    
    def extract_roi(self, ct_image_path, bbox, margin_factor=2.0):
        """
        ä»CTå›¾åƒä¸­æå–ç»“èŠ‚ROI
        
        å‚æ•°:
            ct_image_path: CTå›¾åƒè·¯å¾„
            bbox: è¾¹ç•Œæ¡† [x_min, y_min, z_min, x_max, y_max, z_max]
            margin_factor: è¾¹ç•Œæ‰©å±•å€æ•°
        
        è¿”å›:
            roi: æå–çš„ROIæ•°ç»„
        """
        # åŠ è½½CTå›¾åƒ
        nii_img = nib.load(ct_image_path)
        ct_data = nii_img.get_fdata()
        
        # è®¡ç®—è¾¹ç•Œæ¡†ä¸­å¿ƒå’Œå¤§å°
        x_min, y_min, z_min, x_max, y_max, z_max = bbox
        
        cx = (x_min + x_max) / 2
        cy = (y_min + y_max) / 2
        cz = (z_min + z_max) / 2
        
        w = (x_max - x_min) * margin_factor
        h = (y_max - y_min) * margin_factor
        d = (z_max - z_min) * margin_factor
        
        # ç¡®ä¿æœ€å°ROIå¤§å°
        w = max(w, 20)
        h = max(h, 20)
        d = max(d, 10)
        
        # è®¡ç®—æ–°çš„è¾¹ç•Œ
        x_start = int(max(0, cx - w/2))
        x_end = int(min(ct_data.shape[0], cx + w/2))
        y_start = int(max(0, cy - h/2))
        y_end = int(min(ct_data.shape[1], cy + h/2))
        z_start = int(max(0, cz - d/2))
        z_end = int(min(ct_data.shape[2], cz + d/2))
        
        # æå–ROI
        roi = ct_data[x_start:x_end, y_start:y_end, z_start:z_end]
        
        return roi, (x_start, x_end, y_start, y_end, z_start, z_end)
    
    def extract_features(self, roi_data):
        """
        æå–å¤šå±‚æ·±åº¦ç‰¹å¾ï¼ˆä»æµ…å±‚åˆ°æ·±å±‚ï¼‰
        
        è¿”å›:
            multi_layer_features: åŒ…å«å„å±‚ç‰¹å¾çš„å­—å…¸
            logits: åˆ†ç±»logits
            prob: åˆ†ç±»æ¦‚ç‡
        """
        # æ‰‹åŠ¨é¢„å¤„ç†
        # 1. HUå€¼å½’ä¸€åŒ–
        roi_normalized = np.clip(roi_data, -1024, 300)
        roi_normalized = (roi_normalized + 1024) / 1324.0  # å½’ä¸€åŒ–åˆ°[0, 1]
        
        # 2. è½¬ä¸ºtorch tensorå¹¶æ·»åŠ batchå’Œchannelç»´åº¦
        roi_tensor = torch.from_numpy(roi_normalized).float()
        roi_tensor = roi_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, D, H, W]
        
        # 3. Resizeåˆ°ç›®æ ‡å¤§å°
        roi_tensor = torch.nn.functional.interpolate(
            roi_tensor,
            size=CONFIG['roi_size'],
            mode='trilinear',
            align_corners=False
        )
        
        # ç§»åˆ°è®¾å¤‡
        roi_tensor = roi_tensor.to(self.device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            logits = self.model(roi_tensor)
            prob = torch.softmax(logits, dim=1)
        
        # æå–å¤šå±‚ç‰¹å¾
        multi_layer_features = {}
        
        # avgpoolç‰¹å¾ï¼šæœ€ç»ˆç‰¹å¾å‘é‡ï¼ˆ2048ç»´ï¼‰
        if 'avgpool' in self.features:
            multi_layer_features['avgpool'] = self.features['avgpool'].squeeze().cpu().numpy()
        
        # å¯¹å„å±‚ç‰¹å¾å›¾è¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–ï¼Œå¾—åˆ°ç‰¹å¾å‘é‡
        for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:
            if layer_name in self.features:
                # ç‰¹å¾å›¾å½¢çŠ¶: [1, C, D, H, W]
                feature_map = self.features[layer_name]
                # å…¨å±€å¹³å‡æ± åŒ–: [1, C, D, H, W] -> [1, C]
                pooled_features = torch.mean(feature_map, dim=[2, 3, 4])
                multi_layer_features[layer_name] = pooled_features.squeeze().cpu().numpy()
        
        return multi_layer_features, logits.cpu().numpy(), prob.cpu().numpy()
    
    def get_interpretable_features(self, roi_data, bbox):
        """
        æå–å¯è§£é‡Šçš„ç‰¹å¾
        
        è¿”å›åŒ…å«ä»¥ä¸‹å†…å®¹çš„å­—å…¸:
        - å½¢çŠ¶ç‰¹å¾ï¼šä½“ç§¯ã€çƒå½¢åº¦ã€ç´§å‡‘åº¦
        - å¼ºåº¦ç‰¹å¾ï¼šå¹³å‡HUã€æ ‡å‡†å·®ã€å³°åº¦ã€ååº¦
        - çº¹ç†ç‰¹å¾ï¼šç†µã€å¯¹æ¯”åº¦
        - ä½ç½®ç‰¹å¾ï¼šx,y,zåæ ‡
        """
        features = {}
        
        # 1. å½¢çŠ¶ç‰¹å¾
        volume = np.prod(roi_data.shape)
        features['volume_voxels'] = volume
        
        # è®¡ç®—çƒå½¢åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        bbox_volume = (bbox[3]-bbox[0]) * (bbox[4]-bbox[1]) * (bbox[5]-bbox[2])
        features['sphericity'] = volume / (bbox_volume + 1e-6)
        
        # 2. å¼ºåº¦ç‰¹å¾ï¼ˆHUå€¼ï¼‰
        features['mean_hu'] = float(np.mean(roi_data))
        features['std_hu'] = float(np.std(roi_data))
        features['min_hu'] = float(np.min(roi_data))
        features['max_hu'] = float(np.max(roi_data))
        features['median_hu'] = float(np.median(roi_data))
        
        # ååº¦å’Œå³°åº¦
        from scipy import stats
        features['skewness'] = float(stats.skew(roi_data.flatten()))
        features['kurtosis'] = float(stats.kurtosis(roi_data.flatten()))
        
        # 3. çº¹ç†ç‰¹å¾
        # è®¡ç®—ç†µ
        hist, _ = np.histogram(roi_data, bins=50)
        hist = hist / (hist.sum() + 1e-6)
        entropy = -np.sum(hist * np.log2(hist + 1e-6))
        features['entropy'] = float(entropy)
        
        # è®¡ç®—å¯¹æ¯”åº¦ï¼ˆæ ‡å‡†å·®/å‡å€¼ï¼‰
        features['contrast'] = float(features['std_hu'] / (abs(features['mean_hu']) + 1e-6))
        
        # 4. ä½ç½®ç‰¹å¾
        features['center_x'] = float((bbox[0] + bbox[3]) / 2)
        features['center_y'] = float((bbox[1] + bbox[4]) / 2)
        features['center_z'] = float((bbox[2] + bbox[5]) / 2)
        
        features['width'] = float(bbox[3] - bbox[0])
        features['height'] = float(bbox[4] - bbox[1])
        features['depth'] = float(bbox[5] - bbox[2])
        
        return features

# ==================== ä¸»ç¨‹åº ====================
def main():
    """ä¸»ç¨‹åº"""
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(CONFIG['detection_results']):
        print(f"âŒ æ‰¾ä¸åˆ°detectionç»“æœæ–‡ä»¶: {CONFIG['detection_results']}")
        return
    
    if not os.path.exists(CONFIG['classification_model']):
        print(f"âŒ æ‰¾ä¸åˆ°classificationæ¨¡å‹: {CONFIG['classification_model']}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(CONFIG['output_explanations'], exist_ok=True)
    
    # åŠ è½½detectionç»“æœ
    print("\nğŸ“‚ åŠ è½½detectionç»“æœ...")
    with open(CONFIG['detection_results'], 'r', encoding='utf-8') as f:
        detection_data = json.load(f)
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = NoduleFeatureExtractor(CONFIG['classification_model'])
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_features = []
    all_predictions = []
    
    print("\n" + "=" * 80)
    print("ğŸ”¬ å¼€å§‹ç‰¹å¾æå–...")
    print("=" * 80)
    
    nodule_count = 0
    
    # éå†æ‰€æœ‰æ£€æµ‹ç»“æœ
    for scan_idx, scan_result in enumerate(detection_data['validation'], 1):
        image_path = scan_result['image']
        boxes = scan_result['box']
        scores = scan_result['score']
        labels = scan_result['label']
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“èŠ‚
        high_conf_indices = [i for i, s in enumerate(scores) if s >= CONFIG['min_confidence']]
        
        if len(high_conf_indices) == 0:
            continue
        
        print(f"\n[{scan_idx}/59] å¤„ç†: {os.path.basename(image_path)}")
        print(f"  æ£€æµ‹åˆ° {len(boxes)} ä¸ªç»“èŠ‚ï¼Œå…¶ä¸­ {len(high_conf_indices)} ä¸ªç½®ä¿¡åº¦ >= {CONFIG['min_confidence']}")
        
        # å¤„ç†æ¯ä¸ªé«˜ç½®ä¿¡åº¦ç»“èŠ‚
        for idx in high_conf_indices:
            nodule_count += 1
            bbox = boxes[idx]
            score = scores[idx]
            
            try:
                # 1. æå–ROI
                roi_data, roi_bounds = extractor.extract_roi(
                    image_path, 
                    bbox, 
                    margin_factor=CONFIG['margin_factor']
                )
                
                # 2. æå–å¤šå±‚æ·±åº¦å­¦ä¹ ç‰¹å¾
                multi_layer_features, logits, probs = extractor.extract_features(roi_data)
                
                # 3. æå–å¯è§£é‡Šçš„ä¼ ç»Ÿç‰¹å¾
                interpretable_features = extractor.get_interpretable_features(roi_data, bbox)
                
                # 4. åˆ†ç±»é¢„æµ‹
                predicted_class = int(np.argmax(probs))
                malignancy_prob = float(probs[0, 1])  # æ¶æ€§æ¦‚ç‡
                benign_prob = float(probs[0, 0])      # è‰¯æ€§æ¦‚ç‡
                
                # 5. æ•´åˆæ‰€æœ‰ä¿¡æ¯
                result = {
                    'nodule_id': nodule_count,
                    'scan_file': os.path.basename(image_path),
                    'detection_score': float(score),
                    
                    # åˆ†ç±»ç»“æœ
                    'predicted_class': predicted_class,
                    'class_name': 'æ¶æ€§' if predicted_class == 1 else 'è‰¯æ€§',
                    'malignancy_probability': malignancy_prob,
                    'benign_probability': benign_prob,
                    
                    # ä½ç½®ä¿¡æ¯
                    'bbox_x_min': float(bbox[0]),
                    'bbox_y_min': float(bbox[1]),
                    'bbox_z_min': float(bbox[2]),
                    'bbox_x_max': float(bbox[3]),
                    'bbox_y_max': float(bbox[4]),
                    'bbox_z_max': float(bbox[5]),
                }
                
                # æ·»åŠ å¯è§£é‡Šç‰¹å¾
                result.update(interpretable_features)
                
                all_predictions.append(result)
                
                # æ‰“å°ç»“æœ
                print(f"    ç»“èŠ‚#{nodule_count}: {result['class_name']} "
                      f"(æ¶æ€§æ¦‚ç‡: {malignancy_prob:.2%}, æ£€æµ‹ç½®ä¿¡åº¦: {score:.2%})")
                print(f"      - ä½ç½®: ({interpretable_features['center_x']:.1f}, "
                      f"{interpretable_features['center_y']:.1f}, "
                      f"{interpretable_features['center_z']:.1f})")
                print(f"      - å¤§å°: {interpretable_features['width']:.1f} Ã— "
                      f"{interpretable_features['height']:.1f} Ã— "
                      f"{interpretable_features['depth']:.1f} åƒç´ ")
                print(f"      - å¹³å‡HUå€¼: {interpretable_features['mean_hu']:.1f}")
                
                # ä¿å­˜å¤šå±‚æ·±åº¦ç‰¹å¾
                if multi_layer_features:
                    feature_dict = {
                        'nodule_id': nodule_count,
                        'scan_file': os.path.basename(image_path),
                    }
                    
                    # ä¿å­˜å„å±‚ç‰¹å¾çš„ç»´åº¦ä¿¡æ¯å’Œå®é™…ç‰¹å¾
                    for layer_name, features in multi_layer_features.items():
                        feature_dict[f'{layer_name}_dim'] = len(features)
                        feature_dict[f'{layer_name}_features'] = features.tolist()
                    
                    all_features.append(feature_dict)
                
            except Exception as e:
                print(f"    âš ï¸  ç»“èŠ‚å¤„ç†å¤±è´¥: {e}")
                continue
    
    # ä¿å­˜ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    print("=" * 80)
    
    # ä¿å­˜é¢„æµ‹å’Œå¯è§£é‡Šç‰¹å¾
    if all_predictions:
        df_predictions = pd.DataFrame(all_predictions)
        df_predictions.to_csv(CONFIG['output_predictions'], index=False, encoding='utf-8-sig')
        print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {CONFIG['output_predictions']}")
        print(f"   å…± {len(all_predictions)} ä¸ªç»“èŠ‚")
        
        # ç»Ÿè®¡
        print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»ç»“èŠ‚æ•°: {len(all_predictions)}")
        malignant_count = sum([1 for p in all_predictions if p['predicted_class'] == 1])
        benign_count = sum([1 for p in all_predictions if p['predicted_class'] == 0])
        print(f"  é¢„æµ‹ä¸ºæ¶æ€§: {malignant_count} ä¸ª")
        print(f"  é¢„æµ‹ä¸ºè‰¯æ€§: {benign_count} ä¸ª")
        
        # é«˜å±ç»“èŠ‚ï¼ˆæ¶æ€§æ¦‚ç‡>0.5ï¼‰
        high_risk = [p for p in all_predictions if p['malignancy_probability'] > 0.5]
        print(f"  é«˜å±ç»“èŠ‚(æ¶æ€§æ¦‚ç‡>50%): {len(high_risk)} ä¸ª")
        
        if high_risk:
            print("\nâš ï¸  é«˜å±ç»“èŠ‚åˆ—è¡¨:")
            for h in sorted(high_risk, key=lambda x: x['malignancy_probability'], reverse=True)[:10]:
                print(f"    ç»“èŠ‚#{h['nodule_id']}: æ¶æ€§æ¦‚ç‡={h['malignancy_probability']:.2%}, "
                      f"æ£€æµ‹ç½®ä¿¡åº¦={h['detection_score']:.2%}")
                print(f"      æ–‡ä»¶: {h['scan_file']}")
    
    # ä¿å­˜æ·±åº¦ç‰¹å¾
    if all_features:
        with open(CONFIG['output_features'].replace('.csv', '.json'), 'w') as f:
            json.dump(all_features, f, indent=2)
        print(f"\nâœ… æ·±åº¦ç‰¹å¾å·²ä¿å­˜: {CONFIG['output_features'].replace('.csv', '.json')}")
    
    print("\nğŸ‰ ç‰¹å¾æå–å®Œæˆ!")
    print(f"\nğŸ“„ è¾“å‡ºæ–‡ä»¶:")
    print(f"  1. {CONFIG['output_predictions']} - å¯è§£é‡Šç‰¹å¾å’Œé¢„æµ‹ç»“æœ")
    print(f"  2. {CONFIG['output_features'].replace('.csv', '.json')} - å¤šå±‚æ·±åº¦å­¦ä¹ ç‰¹å¾")
    print(f"     â€¢ layer1: 256ç»´ (æµ…å±‚ç‰¹å¾ - è¾¹ç¼˜ã€çº¹ç†)")
    print(f"     â€¢ layer2: 512ç»´ (ä¸­æµ…å±‚ç‰¹å¾ - ç®€å•å½¢çŠ¶)")
    print(f"     â€¢ layer3: 1024ç»´ (ä¸­æ·±å±‚ç‰¹å¾ - å¤æ‚å½¢çŠ¶)")
    print(f"     â€¢ layer4: 2048ç»´ (æ·±å±‚ç‰¹å¾ - é«˜çº§è¯­ä¹‰)")
    print(f"     â€¢ avgpool: 2048ç»´ (æœ€ç»ˆç‰¹å¾å‘é‡)")

# ==================== å¯è§£é‡Šç‰¹å¾è¯´æ˜ ====================
def print_feature_explanation():
    """æ‰“å°ç‰¹å¾è¯´æ˜æ–‡æ¡£"""
    explanation = """
    
ğŸ“š å¯è§£é‡Šç‰¹å¾è¯´æ˜
================================================================================

ğŸ§  æ·±åº¦å­¦ä¹ ç‰¹å¾ï¼ˆå¤šå±‚ResNet50ç‰¹å¾ï¼‰
--------------------------------------------------------------------------------
ç°åœ¨æå–äº†ResNet50çš„å¤šå±‚ç‰¹å¾ï¼Œä»æµ…å±‚åˆ°æ·±å±‚ï¼š

  â€¢ layer1 (256ç»´): æµ…å±‚ç‰¹å¾
    - æ•è·ä½çº§è§†è§‰ä¿¡æ¯ï¼šè¾¹ç¼˜ã€è§’ç‚¹ã€ç®€å•çº¹ç†
    - åˆ†è¾¨ç‡é«˜ï¼Œç©ºé—´ç»†èŠ‚ä¸°å¯Œ
    - é€‚åˆåˆ†æç»“èŠ‚è¾¹ç¼˜çš„é”åˆ©åº¦å’Œçº¹ç†ç»†èŠ‚

  â€¢ layer2 (512ç»´): ä¸­æµ…å±‚ç‰¹å¾
    - æ•è·ç®€å•çš„å½¢çŠ¶å’Œå±€éƒ¨æ¨¡å¼
    - é€‚åˆåˆ†æç»“èŠ‚çš„å±€éƒ¨ç»“æ„

  â€¢ layer3 (1024ç»´): ä¸­æ·±å±‚ç‰¹å¾
    - æ•è·å¤æ‚çš„å½¢çŠ¶å’Œéƒ¨ä»¶ç»„åˆ
    - é€‚åˆåˆ†æç»“èŠ‚çš„æ•´ä½“å½¢çŠ¶ç‰¹å¾

  â€¢ layer4 (2048ç»´): æ·±å±‚ç‰¹å¾
    - æ•è·é«˜çº§è¯­ä¹‰ç‰¹å¾å’ŒæŠ½è±¡è¡¨ç¤º
    - é€‚åˆåŒºåˆ†ç»“èŠ‚ç±»å‹ï¼ˆè‰¯æ€§/æ¶æ€§ï¼‰

  â€¢ avgpool (2048ç»´): æœ€ç»ˆç‰¹å¾å‘é‡
    - layer4ç»è¿‡å…¨å±€å¹³å‡æ± åŒ–åçš„ç‰¹å¾
    - ç”¨äºæœ€ç»ˆåˆ†ç±»å†³ç­–

ğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š
  - æµ…å±‚ç‰¹å¾(layer1, layer2)æ›´é€‚åˆåˆ†æçº¹ç†å’Œè¾¹ç¼˜ç‰¹æ€§
  - æ·±å±‚ç‰¹å¾(layer3, layer4, avgpool)æ›´é€‚åˆåˆ†ç±»å’Œè¯­ä¹‰ç†è§£
  - å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©ä¸åŒå±‚çš„ç‰¹å¾è¿›è¡Œåˆ†æ

================================================================================

1. å½¢çŠ¶ç‰¹å¾ (Shape Features)
   - volume_voxels: ç»“èŠ‚ä½“ç§¯ï¼ˆä½“ç´ æ•°ï¼‰
   - sphericity: çƒå½¢åº¦ï¼ˆ0-1ï¼Œè¶Šæ¥è¿‘1è¶Šåœ†ï¼‰
   - width/height/depth: ç»“èŠ‚ä¸‰ç»´å°ºå¯¸ï¼ˆåƒç´ ï¼‰

2. å¼ºåº¦ç‰¹å¾ (Intensity Features) - åæ˜ ç»“èŠ‚å¯†åº¦
   - mean_hu: å¹³å‡HUå€¼ï¼ˆHounsfieldå•ä½ï¼‰
     * å®æ€§ç»“èŠ‚: é€šå¸¸ > -200 HU
     * ç£¨ç»ç’ƒç»“èŠ‚: -700 åˆ° -200 HU
     * å›Šæ€§ç»“èŠ‚: < -700 HU
   - std_hu: HUå€¼æ ‡å‡†å·®ï¼ˆåæ˜ ç»“èŠ‚å‡åŒ€æ€§ï¼‰
   - min_hu/max_hu: HUå€¼èŒƒå›´
   - median_hu: HUå€¼ä¸­ä½æ•°

3. åˆ†å¸ƒç‰¹å¾ (Distribution Features)
   - skewness: ååº¦ï¼ˆå¯¹ç§°æ€§ï¼‰
     * > 0: å³åï¼ˆé«˜å¯†åº¦åƒç´ è¾ƒå¤šï¼‰
     * < 0: å·¦åï¼ˆä½å¯†åº¦åƒç´ è¾ƒå¤šï¼‰
   - kurtosis: å³°åº¦ï¼ˆåˆ†å¸ƒå°–é”ç¨‹åº¦ï¼‰
     * > 0: å°–å³°åˆ†å¸ƒï¼ˆå¯†åº¦é›†ä¸­ï¼‰
     * < 0: å¹³å¦åˆ†å¸ƒï¼ˆå¯†åº¦åˆ†æ•£ï¼‰

4. çº¹ç†ç‰¹å¾ (Texture Features)
   - entropy: ç†µï¼ˆå¤æ‚åº¦/ä¸è§„åˆ™æ€§ï¼‰
     * é«˜ç†µ: çº¹ç†å¤æ‚ã€ä¸å‡åŒ€
     * ä½ç†µ: çº¹ç†ç®€å•ã€å‡åŒ€
   - contrast: å¯¹æ¯”åº¦ï¼ˆå˜åŒ–ç¨‹åº¦ï¼‰

5. ä½ç½®ç‰¹å¾ (Location Features)
   - center_x/y/z: ç»“èŠ‚ä¸­å¿ƒåæ ‡
   - å¯ç”¨äºåˆ†æç»“èŠ‚åœ¨è‚ºéƒ¨çš„ä½ç½®åˆ†å¸ƒ

6. åˆ†ç±»é¢„æµ‹ (Classification Prediction)
   - malignancy_probability: æ¶æ€§æ¦‚ç‡ï¼ˆ0-1ï¼‰
   - benign_probability: è‰¯æ€§æ¦‚ç‡ï¼ˆ0-1ï¼‰
   - predicted_class: é¢„æµ‹ç±»åˆ«ï¼ˆ0=è‰¯æ€§ï¼Œ1=æ¶æ€§ï¼‰

ğŸ“ˆ ä¸´åºŠè§£é‡ŠæŒ‡å—
================================================================================

é«˜é£é™©ç‰¹å¾ç»„åˆ:
  âœ“ æ¶æ€§æ¦‚ç‡ > 0.5
  âœ“ å¹³å‡HUå€¼ > -100ï¼ˆé«˜å¯†åº¦/å®æ€§ï¼‰
  âœ“ å¤§å° > 8mm
  âœ“ å½¢çŠ¶ä¸è§„åˆ™ï¼ˆä½çƒå½¢åº¦ < 0.6ï¼‰
  âœ“ é«˜ç†µå€¼ï¼ˆçº¹ç†å¤æ‚ï¼‰

ä½é£é™©ç‰¹å¾ç»„åˆ:
  âœ“ æ¶æ€§æ¦‚ç‡ < 0.3
  âœ“ å¹³å‡HUå€¼ < -400ï¼ˆä½å¯†åº¦/ç£¨ç»ç’ƒï¼‰
  âœ“ å¤§å° < 6mm
  âœ“ å½¢çŠ¶è§„åˆ™ï¼ˆé«˜çƒå½¢åº¦ > 0.8ï¼‰
  âœ“ ä½ç†µå€¼ï¼ˆçº¹ç†å‡åŒ€ï¼‰

================================================================================
    """
    print(explanation)

if __name__ == "__main__":
    try:
        main()
        print_feature_explanation()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

